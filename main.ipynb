{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt_model_keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGfiiWUwvvgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Embedding,LSTM,Dropout,Dense,Layer\n",
        "from keras import Model,Input\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import collections\n",
        "import numpy as np\n",
        "import time\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class NmtModel(object):\n",
        "  def __init__(self,source_dict,target_dict,use_attention):\n",
        "    self.hidden_size = 200\n",
        "    self.embedding_size = 100\n",
        "    self.hidden_dropout_rate=0.2\n",
        "    self.embedding_dropout_rate = 0.2\n",
        "    self.batch_size = 100\n",
        "    self.max_target_step = 30\n",
        "    self.vocab_target_size = len(target_dict.vocab)\n",
        "    self.vocab_source_size = len(source_dict.vocab)\n",
        "    self.target_dict = target_dict\n",
        "    self.source_dict = source_dict\n",
        "    self.SOS = target_dict.word2ids['<start>']\n",
        "    self.EOS = target_dict.word2ids['<end>']\n",
        "    self.use_attention = use_attention\n",
        "\n",
        "    print(\"source vocab: %d, target vocab:%d\" % (self.vocab_source_size,self.vocab_target_size))\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    source_words = Input(shape=(None,),dtype='int32')\n",
        "    target_words = Input(shape=(None,), dtype='int32')\n",
        "\n",
        "\n",
        "    #create embedding layers\n",
        "    embedding_source = Embedding(input_dim= self.vocab_source_size, output_dim = self.embedding_size, mask_zero = True)\n",
        "    embedding_target = Embedding(input_dim= self.vocab_target_size, output_dim = self.embedding_size, mask_zero = True)\n",
        "\n",
        "    #get embeddings for inputs\n",
        "    source_words_embeddings = embedding_source(source_words)\n",
        "    target_words_embeddings = embedding_target(target_words)\n",
        "\n",
        "    #create dropout Layers\n",
        "    dropSource = keras.layers.Dropout(self.embedding_dropout_rate)\n",
        "    dropTarget = keras.layers.Dropout(self.embedding_dropout_rate)\n",
        "   \n",
        "    #apply dropout to embeddings\n",
        "    dropped_source_words_embeddings = dropSource(source_words_embeddings)\n",
        "    dropped_target_words_embeddings = dropSource(target_words_embeddings)\n",
        "\n",
        "    #create LSTM layer\n",
        "    LSTM_layer = keras.layers.LSTM(self.hidden_size, return_state=True, return_sequences= True)\n",
        "    \n",
        "    #pass in source encodings\n",
        "    encoder_outputs, encoder_state_h, encoder_state_c = LSTM_layer(dropped_source_words_embeddings)\n",
        "    \n",
        "    encoder_states = [encoder_state_h,encoder_state_c]\n",
        "    \n",
        "    decoder_lstm = LSTM(self.hidden_size,recurrent_dropout=self.hidden_dropout_rate,return_sequences=True,return_state=True)\n",
        "    decoder_outputs_train,_,_ = decoder_lstm(target_words_embeddings,initial_state=encoder_states)\n",
        "\n",
        "\n",
        "    if self.use_attention:\n",
        "      decoder_attention = AttentionLayer()\n",
        "      decoder_outputs_train = decoder_attention([encoder_outputs,decoder_outputs_train])\n",
        "\n",
        "    decoder_dense = Dense(self.vocab_target_size,activation='softmax')\n",
        "    decoder_outputs_train = decoder_dense(decoder_outputs_train)\n",
        "\n",
        "    adam = Adam(lr=0.01,clipnorm=5.0)\n",
        "    self.train_model = Model([source_words,target_words], decoder_outputs_train)\n",
        "    self.train_model.compile(optimizer=adam,loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    self.train_model.summary()\n",
        "\n",
        "    #Inference Models\n",
        "\n",
        "    self.encoder_model = Model(source_words,[encoder_outputs,encoder_state_h,encoder_state_c])\n",
        "    self.encoder_model.summary()\n",
        "\n",
        "    decoder_state_input_h = Input(shape=(self.hidden_size,))\n",
        "    decoder_state_input_c = Input(shape=(self.hidden_size,))\n",
        "    encoder_outputs_input = Input(shape=(None,self.hidden_size,))\n",
        "\n",
        "    # create list of decoder state inuts\n",
        "    decoder_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    #pass the decoder states and target_word embeddings into the decoder LSTM\n",
        "    decoder_outputs_test, decoder_state_output_h, decoder_state_output_c = decoder_lstm(target_words_embeddings, decoder_states)\n",
        "\n",
        "\n",
        "    # use attention layer?\n",
        "    if self.use_attention:\n",
        "      #pass the encoder outputs and decoder outputs into the attention layer\n",
        "      decoder_outputs_test = decoder_attention([encoder_outputs_input, decoder_outputs_test])\n",
        "\n",
        "    #pass the decoder outputs through the decoder's dense layer\n",
        "    decoder_outputs_test = decoder_dense(decoder_outputs_test) \n",
        "    \n",
        "\n",
        "    self.decoder_model = Model([target_words,decoder_state_input_h,decoder_state_input_c,encoder_outputs_input],\n",
        "                               [decoder_outputs_test,decoder_state_output_h,decoder_state_output_c])\n",
        "    self.decoder_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "  def time_used(self, start_time):\n",
        "    curr_time = time.time()\n",
        "    used_time = curr_time-start_time\n",
        "    m = used_time // 60\n",
        "    s = used_time - 60 * m\n",
        "    return \"%d m %d s\" % (m, s)\n",
        "\n",
        "  def train(self,train_data,dev_data,test_data, epochs):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "      print(\"Starting training epoch {}/{}\".format(epoch + 1, epochs))\n",
        "      epoch_time = time.time()\n",
        "      source_words_train, target_words_train, target_words_train_labels = train_data\n",
        "\n",
        "      self.train_model.fit([source_words_train,target_words_train],target_words_train_labels,batch_size=self.batch_size)\n",
        "\n",
        "      print(\"Time used for epoch {}: {}\".format(epoch + 1, self.time_used(epoch_time)))\n",
        "      dev_time = time.time()\n",
        "      print(\"Evaluating on dev set after epoch {}/{}:\".format(epoch + 1, epochs))\n",
        "      self.eval(dev_data)\n",
        "      print(\"Time used for evaluate on dev set: {}\".format(self.time_used(dev_time)))\n",
        "\n",
        "    print(\"Training finished!\")\n",
        "    print(\"Time used for training: {}\".format(self.time_used(start_time)))\n",
        "\n",
        "    print(\"Evaluating on test set:\")\n",
        "    test_time = time.time()\n",
        "    self.eval(test_data)\n",
        "    print(\"Time used for evaluate on test set: {}\".format(self.time_used(test_time)))\n",
        "\n",
        "\n",
        "\n",
        "  def get_target_sentences(self, sents,vocab,reference=False):\n",
        "    str_sents = []\n",
        "    num_sent, max_len = sents.shape\n",
        "    for i in range(num_sent):\n",
        "      str_sent = []\n",
        "      for j in range(max_len):\n",
        "        t = sents[i,j].item()\n",
        "        if t == self.SOS:\n",
        "          continue\n",
        "        if t == self.EOS:\n",
        "          break\n",
        "\n",
        "        str_sent.append(vocab[t])\n",
        "      if reference:\n",
        "        str_sents.append([str_sent])\n",
        "      else:\n",
        "        str_sents.append(str_sent)\n",
        "    return str_sents\n",
        "\n",
        "\n",
        "  def eval(self, dataset):\n",
        "    source_words, target_words_labels = dataset\n",
        "    vocab = self.target_dict.vocab\n",
        "\n",
        "    encoder_outputs, state_h,state_c = self.encoder_model.predict(source_words,batch_size=self.batch_size)\n",
        "    predictions = []\n",
        "    step_target_words = np.ones([source_words.shape[0],1]) * self.SOS\n",
        "    for _ in range(self.max_target_step):\n",
        "      step_decoder_outputs, state_h,state_c = self.decoder_model.predict([step_target_words,state_h,state_c,encoder_outputs],batch_size=self.batch_size)\n",
        "      step_target_words = np.argmax(step_decoder_outputs,axis=2)\n",
        "      predictions.append(step_target_words)\n",
        "\n",
        "    candidates = self.get_target_sentences(np.concatenate(predictions,axis=1),vocab)\n",
        "    \n",
        "    references = self.get_target_sentences(target_words_labels,vocab,reference=True)\n",
        "\n",
        "    '''\n",
        "    print(candidates[1])\n",
        "    print(references[1])\n",
        "\n",
        "    print(candidates[2])\n",
        "    print(references[2])\n",
        "\n",
        "    print(candidates[3])\n",
        "    print(references[3])\n",
        "\n",
        "    score = corpus_bleu(references,candidates)\n",
        "    print(\"Model BLEU score: %.2f\" % (score*100.0))\n",
        "    '''\n",
        "class AttentionLayer(Layer):\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    if mask == None:\n",
        "      return None\n",
        "    return mask[1]\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[1][0],input_shape[1][1],input_shape[1][2]*2)\n",
        "\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    encoder_outputs, decoder_outputs = inputs\n",
        "\n",
        "\n",
        "    #reorder the dimensions of the tensor\n",
        "    decoder_outputs_permuted = keras.backend.permute_dimensions(decoder_outputs, (0,2,1))\n",
        "\n",
        "    #multiply the encoder outputs and reordered decoder_outputs\n",
        "    luong_score = keras.backend.batch_dot(encoder_outputs, decoder_outputs_permuted)  \n",
        "\n",
        "    #apply softmax the max_source_sent_len dimension  \n",
        "    luong_score = keras.activations.softmax(luong_score, 1)    \n",
        "    \n",
        "    #expand the dimensions of the luong score at axis 3\n",
        "    luong_score = keras.backend.expand_dims(luong_score, 3)\n",
        "\n",
        "    #expand the dimesnions of the encoder_outputs at axis 2\n",
        "    encoder_outputs_expanded = keras.backend.expand_dims(encoder_outputs, 2)\n",
        "\n",
        "    #multiply the two expanded tensors\n",
        "    expanded_dims = luong_score * encoder_outputs_expanded\n",
        "\n",
        "    #sum the expanded dimensions at axis 1\n",
        "    encoder_vector = K.sum(expanded_dims, axis=1)\n",
        "\n",
        "\n",
        "    # [batch,max_dec,2*emb]\n",
        "    new_decoder_outputs = K.concatenate([decoder_outputs, encoder_vector])\n",
        "\n",
        "    print(new_decoder_outputs)\n",
        "    return new_decoder_outputs\n",
        "\n",
        "\n",
        "class LanguageDict():\n",
        "  def __init__(self, sents):\n",
        "    word_counter = collections.Counter(tok.lower() for sent in sents for tok in sent)\n",
        "\n",
        "    self.vocab = []\n",
        "    self.vocab.append('<pad>') #zero paddings\n",
        "    self.vocab.append('<unk>')\n",
        "    self.vocab.extend([t for t,c in word_counter.items() if c > 10])\n",
        "\n",
        "    self.word2ids = {w:id for id, w in enumerate(self.vocab)}\n",
        "    self.UNK = self.word2ids['<unk>']\n",
        "    self.PAD = self.word2ids['<pad>']\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(source_path,target_path, max_num_examples=30000):\n",
        "  source_lines = open(source_path).readlines()\n",
        "  target_lines = open(target_path).readlines()\n",
        "  assert len(source_lines) == len(target_lines)\n",
        "  if max_num_examples > 0:\n",
        "    max_num_examples = min(len(source_lines), max_num_examples)\n",
        "    source_lines = source_lines[:max_num_examples]\n",
        "    target_lines = target_lines[:max_num_examples]\n",
        "\n",
        "  source_sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in source_lines]\n",
        "  target_sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in target_lines]\n",
        "  for sent in target_sents:\n",
        "    sent.append('<end>')\n",
        "    sent.insert(0,'<start>')\n",
        "\n",
        "  source_lang_dict = LanguageDict(source_sents)\n",
        "  target_lang_dict = LanguageDict(target_sents)\n",
        "\n",
        "  unit = len(source_sents)//10\n",
        "\n",
        "  source_words = [[source_lang_dict.word2ids.get(tok,source_lang_dict.UNK) for tok in sent] for sent in source_sents]\n",
        "  source_words_train = pad_sequences(source_words[:8*unit],padding='post')\n",
        "  source_words_dev = pad_sequences(source_words[8*unit:9*unit],padding='post')\n",
        "  source_words_test = pad_sequences(source_words[9*unit:],padding='post')\n",
        "\n",
        "  eos = target_lang_dict.word2ids['<end>']\n",
        "\n",
        "  target_words = [[target_lang_dict.word2ids.get(tok,target_lang_dict.UNK) for tok in sent[:-1]] for sent in target_sents]\n",
        "  target_words_train = pad_sequences(target_words[:8*unit],padding='post')\n",
        "\n",
        "  target_words_train_labels = [sent[1:]+[eos] for sent in target_words[:8*unit]]\n",
        "  target_words_train_labels = pad_sequences(target_words_train_labels,padding='post')\n",
        "  target_words_train_labels = np.expand_dims(target_words_train_labels,axis=2)\n",
        "\n",
        "  target_words_dev_labels = pad_sequences([sent[1:] + [eos] for sent in target_words[8 * unit:9 * unit]], padding='post')\n",
        "  target_words_test_labels = pad_sequences([sent[1:] + [eos] for sent in target_words[9 * unit:]], padding='post')\n",
        "\n",
        "  train_data = [source_words_train,target_words_train,target_words_train_labels]\n",
        "  dev_data = [source_words_dev,target_words_dev_labels]\n",
        "  test_data = [source_words_test,target_words_test_labels]\n",
        "\n",
        "  return train_data,dev_data,test_data,source_lang_dict,target_lang_dict\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  max_example = 30000\n",
        "  use_attention = True\n",
        "  train_data, dev_data, test_data, source_dict, target_dict = load_dataset(\"data.30.vi\",\"data.30.en\",max_num_examples=max_example)\n",
        "  print(\"read %d/%d/%d train/dev/test batches\" % (len(train_data[0]),len(dev_data[0]), len(test_data[0])))\n",
        "\n",
        "  model = NmtModel(source_dict,target_dict,use_attention)\n",
        "  model.build()\n",
        "  model.train(train_data,dev_data,test_data,10)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYoBeqFD_sj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}